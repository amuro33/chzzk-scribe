import sys
import os
import argparse
import time
import io
import json

# Force UTF-8 encoding for stdout and stderr
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# HuggingFace Hub ê²½ê³  ìˆ¨ê¸°ê¸°
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
# ì¸ì¦ ì—†ì´ ì‚¬ìš© (ì¼ë°˜ ì‚¬ìš©ììš©)
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='huggingface_hub')

# ì˜ì¡´ì„± í™•ì¸
try:
    from faster_whisper import WhisperModel
    import torch
except ImportError as e:
    print(f"Error: Required library not found: {e}", file=sys.stderr)
    print("Please install: pip install faster-whisper torch", file=sys.stderr)
    sys.exit(1)

def format_timestamp(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds - int(seconds)) * 1000)
    return f"{hours:02}:{minutes:02}:{secs:02},{millis:03}"

def log_message(message, level="INFO"):
    """ë¡œê·¸ ë©”ì‹œì§€ë¥¼ êµ¬ì¡°í™”ëœ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    log_data = {
        "type": "log",
        "level": level,
        "message": message,
        "timestamp": time.time()
    }
    print(json.dumps(log_data, ensure_ascii=False), flush=True)

def log_progress(progress, stage="transcribing"):
    """ì§„í–‰ë¥ ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (0-1 ë²”ìœ„)"""
    progress_data = {
        "type": "progress",
        "stage": stage,
        "progress": min(max(progress, 0.0), 1.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
    }
    print(json.dumps(progress_data, ensure_ascii=False), flush=True)

def detect_gpu():
    """NVIDIA GPU ê°ì§€ - ë°°í¬ í™˜ê²½ ì•ˆì „ì„± ìš°ì„ """
    import sys
    
    # ìƒì„¸ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    sys.stderr.write("=" * 60 + "\n")
    sys.stderr.write("ğŸ” GPU ê°ì§€ ë””ë²„ê¹… ì‹œì‘\n")
    sys.stderr.write("=" * 60 + "\n")
    sys.stderr.write(f"PyTorch ë²„ì „: {torch.__version__}\n")
    
    # CPU ë²„ì „ì¸ì§€ ë¨¼ì € ì²´í¬
    is_cpu_only = '+cpu' in torch.__version__
    if is_cpu_only:
        sys.stderr.write("âš ï¸ âš ï¸ âš ï¸ PyTorch CPU ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤! âš ï¸ âš ï¸ âš ï¸\n")
        sys.stderr.write("GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ GPU ë²„ì „ìœ¼ë¡œ ì¬ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n")
        sys.stderr.write("ì„¤ì • í™”ë©´ì—ì„œ ì—”ì§„ì„ ì‚­ì œ í›„ GPU ë²„ì „ìœ¼ë¡œ ë‹¤ì‹œ ì„¤ì¹˜í•˜ì„¸ìš”.\n")
    
    sys.stderr.write(f"CUDA ë¹Œë“œ í¬í•¨ ì—¬ë¶€: {torch.cuda.is_available()}\n")
    
    try:
        if hasattr(torch.version, 'cuda') and torch.version.cuda:
            sys.stderr.write(f"PyTorch CUDA ë²„ì „: {torch.version.cuda}\n")
        else:
            sys.stderr.write("âš ï¸ PyTorchê°€ CPU ì „ìš© ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
    except:
        pass
    
    try:
        cuda_available = torch.cuda.is_available()
        sys.stderr.write(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}\n")
        
        if cuda_available and not is_cpu_only:
            device_count = torch.cuda.device_count()
            sys.stderr.write(f"ê°ì§€ëœ GPU ê°œìˆ˜: {device_count}\n")
            
            for i in range(device_count):
                device_name = torch.cuda.get_device_name(i)
                sys.stderr.write(f"GPU {i}: {device_name}\n")
            
            sys.stderr.write("=" * 60 + "\n")
            log_message(f"âœ“ NVIDIA GPU ê°ì§€: {torch.cuda.get_device_name(0)}", "INFO")
            return True, torch.cuda.get_device_name(0)
        else:
            if is_cpu_only:
                sys.stderr.write("âŒ PyTorch CPU ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
            else:
                sys.stderr.write("âš ï¸ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n")
                sys.stderr.write("ê°€ëŠ¥í•œ ì›ì¸:\n")
                sys.stderr.write("  1. PyTorchê°€ CPU ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜ë¨ (ê°€ì¥ í”í•¨)\n")
                sys.stderr.write("  2. NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ\n")
                sys.stderr.write("  3. CUDA Toolkit ë¯¸ì„¤ì¹˜\n")
            sys.stderr.write("=" * 60 + "\n")
            log_message("âš ï¸ GPU ì—†ìŒ - CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.", "WARNING")
            return False, None
    except Exception as e:
        sys.stderr.write(f"âŒ GPU ì²´í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
        sys.stderr.write("=" * 60 + "\n")
        log_message(f"GPU ì²´í¬ ì‹¤íŒ¨ ({e}) - CPUë¡œ ì•ˆì „ ì‹¤í–‰", "WARNING")
        return False, None

def main():
    parser = argparse.ArgumentParser(description="Faster-Whisper Transcription Wrapper")
    parser.add_argument("--input", required=True, help="Input media file path")
    parser.add_argument("--model", required=True, help="Model path (e.g., /path/to/small-ct2)")
    parser.add_argument("--device", default="auto", help="Device to use (cuda, cpu, auto)")
    parser.add_argument("--output_dir", required=True, help="Directory to save the SRT file")
    parser.add_argument("--language", default="auto", help="Language code (auto, ko, en, ja, etc.)")
    
    args = parser.parse_args()

    # ì¦‰ì‹œ ì‹œì‘ ì‹ í˜¸ ì „ì†¡
    log_progress(0.0, "starting")
    
    input_file = args.input
    model_path = args.model
    output_dir = args.output_dir
    language = args.language if args.language != "auto" else None

    log_message(f"ì‘ì—… ì‹œì‘: {os.path.basename(input_file)}")
    log_message(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
    log_progress(0.02, "initializing")

    # 1. Device Selection Logic (ë°°í¬ í™˜ê²½ ì•ˆì „ì„± ìš°ì„ )
    gpu_available, gpu_name = detect_gpu()
    
    if args.device == "auto":
        # ìë™ ê°ì§€ ëª¨ë“œ (ê¶Œì¥)
        if gpu_available:
            device = "cuda"
            compute_type = "float16"  # NVIDIA GPU ìµœì í™”
            log_message(f"ğŸš€ GPU ê°€ì† í™œì„±í™”: {gpu_name}")
        else:
            device = "cpu"
            compute_type = "int8"  # CPU ë¶€í•˜ ìµœì†Œí™” (í•„ìˆ˜)
            log_message("ğŸ’» CPU ëª¨ë“œ (ì•ˆì •ì„± ìš°ì„ )")
    elif args.device == "cuda":
        if gpu_available:
            device = "cuda"
            compute_type = "float16"
            log_message(f"ì‚¬ìš©ì ì§€ì •: GPU ëª¨ë“œ ({gpu_name})")
        else:
            log_message("âš ï¸ GPUê°€ ì—†ì–´ CPUë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.", "WARNING")
            device = "cpu"
            compute_type = "int8"
    else:
        device = "cpu"
        compute_type = "int8"
        log_message("ì‚¬ìš©ì ì§€ì •: CPU ëª¨ë“œ")

    # 2. Load Model
    log_message("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    log_progress(0.05, "loading_model")  # 5% - ëª¨ë¸ ë¡œë”© ì‹œì‘
    
    try:
        model = WhisperModel(model_path, device=device, compute_type=compute_type)
        log_message("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        log_progress(0.1, "model_loaded")  # 10% - ëª¨ë¸ ë¡œë”© ì™„ë£Œ
    except Exception as e:
        log_message(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({device}): {e}", "ERROR")
        
        # CUDA ì‹¤íŒ¨ ì‹œ CPUë¡œ í´ë°±
        if device == "cuda":
            log_message("âš ï¸ CPU ëª¨ë“œë¡œ ì¬ì‹œë„...", "WARNING")
            device = "cpu"
            compute_type = "int8"
            try:
                model = WhisperModel(model_path, device=device, compute_type=compute_type)
                log_message("âœ… CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                log_progress(0.1, "model_loaded")
            except Exception as e2:
                log_message(f"âŒ CPU ëª¨ë“œì—ì„œë„ ì‹¤íŒ¨: {e2}", "ERROR")
                sys.exit(1)
        else:
            sys.exit(1)

    # 3. Transcribe
    log_message(f"ğŸ™ï¸ ìŒì„± ì¸ì‹ ì‹œì‘...")
    log_progress(0.15, "preparing")  # 15% - ì¤€ë¹„ ì¤‘
    
    # ì–¸ì–´ ê°ì§€
    if not language:
        log_message("ì–¸ì–´ ìë™ ê°ì§€ ì¤‘...")
    
    # ì¹˜ì§€ì§ ìŠ¤íŠ¸ë¦¬ë° ë°©ì†¡ ìµœì í™” ì„¤ì •
    initial_prompt = "ì´ ì˜ìƒì€ í•œêµ­ì–´ ê²Œì„ ë°©ì†¡ ë° ìŠ¤íŠ¸ë¦¬ë° ì½˜í…ì¸ ì…ë‹ˆë‹¤."
    
    log_message("ğŸ” ì˜ìƒ ë¶„ì„ ì¤‘... (ì²« ì„¸ê·¸ë¨¼íŠ¸ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    log_progress(0.2, "analyzing")  # 20% - ë¶„ì„ ì¤‘
    
    # GPU ì‚¬ìš© ì‹œ cuDNN ì—ëŸ¬ ë°œìƒí•˜ë©´ CPUë¡œ ì¬ì‹œë„
    transcribe_success = False
    segments = None
    info = None
    
    try:
        segments, info = model.transcribe(
            input_file,
            beam_size=5,
            language=language,
            vad_filter=True,  # í•„ìˆ˜: ê²Œì„ ì†Œë¦¬/ë°°ê²½ìŒì•… êµ¬ê°„ ì œê±°
            vad_parameters=dict(
                min_silence_duration_ms=500,  # 0.5ì´ˆ ì´ìƒ ë¬´ìŒë§Œ ì œê±°
                threshold=0.5  # VAD ë¯¼ê°ë„
            ),
            word_timestamps=True,  # ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •í™•í•œ ì‹œì‘/ì¢…ë£Œ ì‹œì  ê°ì§€
            initial_prompt=initial_prompt,  # í•œêµ­ì–´ ì¸ì‹ë¥  í–¥ìƒ (ì½œë“œ ìŠ¤íƒ€íŠ¸ ë°©ì§€)
            condition_on_previous_text=True  # ë¬¸ë§¥ ìœ ì§€
        )
        transcribe_success = True
    except Exception as transcribe_error:
        error_msg = str(transcribe_error)
        log_message(f"âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨ ({device}): {error_msg}", "ERROR")
        
        # cuDNN ì—ëŸ¬ ë˜ëŠ” CUDA ì—ëŸ¬ ì‹œ CPUë¡œ ì¬ì‹œë„
        if device == "cuda" and ("cudnn" in error_msg.lower() or "cuda" in error_msg.lower()):
            log_message("âš ï¸ GPU ì‹¤í–‰ ì‹¤íŒ¨ - CPU ëª¨ë“œë¡œ ì¬ì‹œë„ ì¤‘...", "WARNING")
            log_progress(0.05, "loading_model")
            
            try:
                # CPU ëª¨ë“œë¡œ ëª¨ë¸ ì¬ë¡œë”©
                device = "cpu"
                compute_type = "int8"
                model = WhisperModel(model_path, device=device, compute_type=compute_type)
                log_message("âœ… CPU ëª¨ë“œë¡œ ëª¨ë¸ ì¬ë¡œë”© ì™„ë£Œ")
                log_progress(0.15, "preparing")
                
                # CPUë¡œ ë‹¤ì‹œ ì‹œë„
                log_message("ğŸ” CPUë¡œ ì˜ìƒ ë¶„ì„ ì¤‘...")
                log_progress(0.2, "analyzing")
                
                segments, info = model.transcribe(
                    input_file,
                    beam_size=5,
                    language=language,
                    vad_filter=True,
                    vad_parameters=dict(
                        min_silence_duration_ms=500,
                        threshold=0.5
                    ),
                    word_timestamps=True,
                    initial_prompt=initial_prompt,
                    condition_on_previous_text=True
                )
                transcribe_success = True
                log_message("âœ… CPU ëª¨ë“œë¡œ ìŒì„± ì¸ì‹ ì„±ê³µ")
            except Exception as cpu_error:
                log_message(f"âŒ CPU ëª¨ë“œì—ì„œë„ ì‹¤íŒ¨: {cpu_error}", "ERROR")
                error_result = {
                    "type": "result",
                    "success": False,
                    "error": f"CPU/GPU ëª¨ë‘ ì‹¤íŒ¨: {cpu_error}"
                }
                print(json.dumps(error_result, ensure_ascii=False), flush=True)
                sys.exit(1)
        else:
            # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì‹¤íŒ¨
            error_result = {
                "type": "result",
                "success": False,
                "error": str(transcribe_error)
            }
            print(json.dumps(error_result, ensure_ascii=False), flush=True)
            sys.exit(1)
    
    if not transcribe_success or segments is None or info is None:
        log_message("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨", "ERROR")
        sys.exit(1)
    
    try:
        
        log_progress(0.22, "analyzing")  # 22% - ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ
        log_message(f"âœ… ì–¸ì–´ ê°ì§€: {info.language} (í™•ë¥ : {info.language_probability:.2f})")
        
        total_duration = info.duration
        log_message(f"ğŸ“Š ì „ì²´ ê¸¸ì´: {total_duration:.1f}ì´ˆ")
        log_progress(0.25, "transcribing")  # 25% - ë³€í™˜ ì‹œì‘
        log_message("ğŸ“ ìë§‰ íŒŒì¼ ìƒì„± ì¤‘... (ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘)")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        srt_path = os.path.join(output_dir, f"{base_name}.srt")
        
        # SRT íŒŒì¼ ìƒì„±
        segment_count = 0
        last_reported_percent = 25  # 25%ë¶€í„° ì‹œì‘
        first_segment = True
        
        with open(srt_path, "w", encoding="utf-8") as srt_file:
            for i, segment in enumerate(segments, start=1):
                # ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë°›ì•˜ì„ ë•Œ ì•Œë¦¼
                if first_segment:
                    log_message("âœ… ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì™„ë£Œ, ë‚˜ë¨¸ì§€ ì²˜ë¦¬ ì¤‘...")
                    log_progress(0.28, "transcribing")
                    first_segment = False
                # word_timestampsë¥¼ í™œìš©í•´ ì‹¤ì œ ì²«/ë§ˆì§€ë§‰ ë‹¨ì–´ ì‹œì  ì‚¬ìš©
                if hasattr(segment, 'words') and segment.words and len(segment.words) > 0:
                    start_time = format_timestamp(segment.words[0].start)
                    end_time = format_timestamp(segment.words[-1].end)
                else:
                    start_time = format_timestamp(segment.start)
                    end_time = format_timestamp(segment.end)
                
                text = segment.text.strip()
                
                srt_file.write(f"{i}\n{start_time} --> {end_time}\n{text}\n\n")
                
                segment_count += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (25%~95% ë²”ìœ„ ì‚¬ìš©)
                if total_duration > 0:
                    # 25%ë¶€í„° 95%ê¹Œì§€ ë§¤í•‘
                    raw_progress = segment.end / total_duration
                    current_progress = 0.25 + (raw_progress * 0.70)  # 25% + (0~100% * 70%) = 25~95%
                    current_percent = int(current_progress * 100)
                    
                    # 2% ì´ìƒ ë³€í™”í–ˆê±°ë‚˜ ë§¤ 3ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ ì—…ë°ì´íŠ¸ (ë” ìì£¼)
                    if (current_percent >= last_reported_percent + 2) or (segment_count % 3 == 0):
                        log_progress(current_progress, "transcribing")
                        last_reported_percent = current_percent

        log_message(f"âœ… ìë§‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: {srt_path}")
        log_message(f"ğŸ“Š ì´ {segment_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ë¨")
        log_progress(1.0, "completed")  # 100% ì™„ë£Œ
        
        # ì„±ê³µ ê²°ê³¼ ì¶œë ¥
        result = {
            "type": "result",
            "success": True,
            "output_path": srt_path,
            "duration": total_duration,
            "language": info.language
        }
        print(json.dumps(result, ensure_ascii=False), flush=True)

    except Exception as e:
        log_message(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}", "ERROR")
        error_result = {
            "type": "result",
            "success": False,
            "error": str(e)
        }
        print(json.dumps(error_result, ensure_ascii=False), flush=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
